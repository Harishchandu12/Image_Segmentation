# -*- coding: utf-8 -*-
"""Mask_R_CNN_using_Detectron2_(Instance_Segmentation).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F0E0Gk9Mw1vBigZ3qht7KeqjLYiXhtPS

Prerequisites (Install & Setup)
"""

!git clone https://github.com/facebookresearch/detectron2.git
!pip install -e detectron2

import torch
print(torch.__version__)
print(torch.version.cuda)

# Step 1: Install dependencies
!pip install -U torch torchvision

# Step 2: Install Detectron2 for CUDA 11.8 and PyTorch 2.x
!pip install -q detectron2 -f \
  https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.0/index.html

!pip show detectron2

# Install dependencies
!pip install -U torch torchvision --quiet

# Install Detectron2 - for PyTorch 2.0 & CUDA 11.8 (Colab default)
!pip install -q detectron2 -f \
  https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.0/index.html

# Commented out IPython magic to ensure Python compatibility.
# 2. Install PyTorch and torchvision
!pip install -U torch torchvision --quiet

# 3. Install build dependencies
!apt-get update -qq
!apt-get install -qq ninja-build libomp-dev

# 4. Clone Detectron2 repository
!git clone https://github.com/facebookresearch/detectron2.git

# 5. Install Detectron2 in editable mode (builds from source)
# %cd detectron2
!pip install -e .
# %cd ..

# 6. Install pycocotools (needed for COCO-format data)
!pip install pycocotools --quiet

import torch
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.utils.visualizer import Visualizer
from detectron2.data.datasets import register_coco_instances
from detectron2.data import MetadataCatalog, DatasetCatalog

import os

# Commented out IPython magic to ensure Python compatibility.
# 2. Install PyTorch and torchvision
!pip install -U torch torchvision --quiet

# 3. Install build dependencies
!apt-get update -qq
!apt-get install -qq ninja-build libomp-dev

# 4. Clone Detectron2 repository
!git clone https://github.com/facebookresearch/detectron2.git

# 5. Install Detectron2 in editable mode (builds from source)
# %cd detectron2
!pip install -e .
# %cd ..

# 6. Install pycocotools (needed for COCO-format data)
!pip install pycocotools --quiet

from google.colab import drive
drive.mount('/content/drive')

# Change paths to dataset location
train_dir = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300"
val_dir = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300"
test_dir = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/test-30"

register_coco_instances("coco_train", {}, os.path.join(train_dir, "annotations.json"), train_dir)
register_coco_instances("coco_val", {}, os.path.join(val_dir, "annotations.json"), val_dir)

MetadataCatalog.get("coco_train").thing_classes = ["cake", "car", "dog", "person"]

!pip install -U torchvision

from zipfile import ZipFile
import os

# Unzip train, val, and test zips
for zip_file in ["/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300.zip", "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300.zip", "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/test-30.zip"]:
    with ZipFile(zip_file, 'r') as zip_ref:
        zip_ref.extractall(f"/content/{zip_file[:-4]}")

"""STEP 1: Install Required Libraries"""

!pip install pycocotools
!pip install torchvision
!pip install matplotlib

"""STEP 2: Mount Drive & Unzip Dataset (If from Google Drive)"""

from zipfile import ZipFile
import os

# Upload files using Colab's uploader or copy them from Drive
zip_paths = {
    "train": "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300.zip",
    "val": "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300.zip",
    "test": "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/test-30.zip"
}

# Unzip each dataset
for name, path in zip_paths.items():
    with ZipFile(path, 'r') as zip_ref:
        zip_ref.extractall(f'/content/{name}')

"""STEP 3: Filter and Prepare COCO Annotations for 4 Classes"""

import json

# Define the category names you are interested in
target_classes = ['cake', 'car', 'dog', 'person']

def filter_coco_json(json_path, target_classes):
    with open(json_path) as f:
        data = json.load(f)

    categories = [cat for cat in data['categories'] if cat['name'] in target_classes]
    category_ids = [cat['id'] for cat in categories]

    filtered_annotations = [ann for ann in data['annotations'] if ann['category_id'] in category_ids]
    image_ids = list(set([ann['image_id'] for ann in filtered_annotations]))
    filtered_images = [img for img in data['images'] if img['id'] in image_ids]

    filtered_data = {
        "images": filtered_images,
        "annotations": filtered_annotations,
        "categories": categories
    }

    return filtered_data

filtered_train_data = filter_coco_json('/content/train/train-300/labels.json', target_classes)
filtered_val_data = filter_coco_json('/content/val/validation-300/labels.json', target_classes)

"""STEP 4: Define a Custom Dataset Class"""

from pycocotools.coco import COCO
import torch
from torch.utils.data import Dataset
from PIL import Image
import torchvision.transforms as T
import os

class CocoDataset(Dataset):
    def __init__(self, image_dir, annotation_data, transforms=None):
        self.image_dir = image_dir
        self.transforms = transforms
        self.annotation_data = annotation_data
        self.images = annotation_data['images']
        self.annotations = annotation_data['annotations']
        self.categories = annotation_data['categories']
        self.category_map = {cat['id']: cat['name'] for cat in self.categories}

        # Map image_id to annotations
        self.image_to_anns = {}
        for ann in self.annotations:
            self.image_to_anns.setdefault(ann['image_id'], []).append(ann)

    def __getitem__(self, idx):
        image_info = self.images[idx]
        img_path = os.path.join(self.image_dir, image_info['file_name'])
        image = Image.open(img_path).convert("RGB")
        image_id = image_info['id']

        # Get annotations for this image
        anns = self.image_to_anns.get(image_id, [])

        boxes, labels, masks = [], [], []
        for ann in anns:
            bbox = ann['bbox']  # [x,y,width,height]
            boxes.append([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]])
            labels.append(ann['category_id'])

            rle = ann['segmentation']
            mask = COCOmask.decode(rle)
            masks.append(mask)

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)
        masks = torch.as_tensor(masks, dtype=torch.uint8)

        target = {
            'boxes': boxes,
            'labels': labels,
            'masks': masks,
            'image_id': torch.tensor([image_id])
        }

        if self.transforms:
            image = self.transforms(image)

        return image, target

    def __len__(self):
        return len(self.images)

""" STEP 5: Load Mask R-CNN Pretrained Model"""

import torchvision
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor

def get_instance_segmentation_model(num_classes):
    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)

    # Replace the classifier head
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)

    # Replace the mask predictor head
    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)

    return model

"""STEP 6: Train the Model

We need to download the `engine.py` script to provide the training and evaluation functions.
"""

!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py

!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py

"""We also need to download the `utils.py` script, as it is used by `engine.py`."""

from torchvision.models.detection import maskrcnn_resnet50_fpn
from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights

def get_instance_segmentation_model(num_classes):
    # Use the DEFAULT pretrained weights
    weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT
    model = maskrcnn_resnet50_fpn(weights=weights)

    # Replace box predictor
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)

    # Replace mask predictor
    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)

    return model

# Step 1: Clone torchvision GitHub repo
!git clone https://github.com/pytorch/vision.git

# Step 2: Copy all necessary scripts into working directory
!cp vision/references/detection/*.py .

import zipfile
import os

# Define paths for your zip files
zip_paths = {
    "train": "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300.zip",
    "val": "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300.zip",
    "test": "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/test-30.zip"
}

# Extract each zip into its own folder
for name, zip_path in zip_paths.items():
    extract_dir = f"/content/{name}"
    os.makedirs(extract_dir, exist_ok=True)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

class COCODataset(torch.utils.data.Dataset):
    def __init__(self, image_dir, annotation_file, transforms=None):
        self.image_dir = image_dir
        self.coco = COCO(annotation_file)
        self.image_ids = list(self.coco.imgs.keys())
        self.transforms = transforms

    def __getitem__(self, index):
        image_id = self.image_ids[index]
        img_info = self.coco.loadImgs(image_id)[0]
        img_path = os.path.join(self.image_dir, img_info['/content/drive/MyDrive/RM_Segmentation_Assignment_dataset'])
        img = Image.open(img_path).convert("RGB")
        ...

from pycocotools.coco import COCO
import torch
from torch.utils.data import Dataset
from PIL import Image
import os

class CocoDataset(Dataset):
    def __init__(self, image_dir, annotation_file, target_classes, transforms=None):
        self.image_dir = image_dir
        self.coco = COCO(annotation_file)
        self.ids = list(self.coco.imgs.keys())
        self.transforms = transforms
        self.class_to_idx = {cls: i+1 for i, cls in enumerate(target_classes)}  # +1 for background

    def __getitem__(self, index):
        image_id = self.ids[index]
        ann_ids = self.coco.getAnnIds(imgIds=image_id)
        anns = self.coco.loadAnns(ann_ids)
        img_info = self.coco.loadImgs(image_id)[0]
        img_path = os.path.join(self.image_dir, img_info['/content/drive/MyDrive/RM_Segmentation_Assignment_dataset'])
        image = Image.open(img_path).convert("RGB")

        # Build target
        boxes = []
        labels = []
        masks = []

        for ann in anns:
            cat_name = self.coco.cats[ann['category_id']]['name']
            if cat_name in self.class_to_idx:
                x, y, w, h = ann['bbox']
                boxes.append([x, y, x + w, y + h])
                labels.append(self.class_to_idx[cat_name])
                masks.append(self.coco.annToMask(ann))

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)
        masks = torch.as_tensor(masks, dtype=torch.uint8)

        target = {
            "boxes": boxes,
            "labels": labels,
            "masks": masks,
            "image_id": torch.tensor([image_id])
        }

        if self.transforms:
            image = self.transforms(image)

        return image, target

    def __len__(self):
        return len(self.ids)

import torchvision.transforms as T

transform = T.Compose([
    T.ToTensor()
])

# Define  4 classes
target_classes = ["cake", "car", "dog", "person"]

# Replace with actual .json file paths after unzipping
train_annotation = "/content/train/train-300/labels.json"
val_annotation = "/content/val/validation-300/labels.json"

train_dataset = CocoDataset("/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/train-300", train_annotation, target_classes, transforms=transform)
val_dataset = CocoDataset("/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/validation-300", val_annotation, target_classes, transforms=transform)

from torch.utils.data import DataLoader

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))
val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))

import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor
from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights

def get_instance_segmentation_model(num_classes):
    weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT
    model = maskrcnn_resnet50_fpn(weights=weights)

    # Box predictor
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    # Mask predictor
    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)

    return model

# Initialize model
num_classes = len(target_classes) + 1  # +1 for background
model = get_instance_segmentation_model(num_classes)

import torch
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# Optimizer
import torch.optim as optim
params = [p for p in model.parameters() if p.requires_grad]
optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)

!git clone https://github.com/pytorch/vision.git
!cp vision/references/detection/*.py .

import zipfile
import os

# Map zip filenames to target extract folders
zip_to_folder = {
    "train-300.zip": "train",
    "val-300.zip": "val",
    "test-30.zip": "test"
}

# Unzip each dataset
for zip_file, folder_name in zip_to_folder.items():
    zip_path = f"/content/{zip_file}"
    extract_path = f"/content/{folder_name}"

    if not os.path.exists(extract_path):
        os.makedirs(extract_path, exist_ok=True)
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        print(f"✅ Extracted {zip_file} to {extract_path}")
    else:
        print(f"⚠️ {extract_path} already exists")

import os
import zipfile
import torch
import torchvision.transforms as T
from PIL import Image
import matplotlib.pyplot as plt

# Step 1: Extract test data
zip_path = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/test-30.zip"
test_dir = "/content/test"

if not os.path.exists(test_dir):
    os.makedirs(test_dir, exist_ok=True)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(test_dir)
    print("✅ Test images extracted.")
else:
    print("✅ Test directory already exists.")

# Step 2: Define image transform
test_transform = T.Compose([T.ToTensor()])

# Step 3: Ensure model is in evaluation mode and on correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# Step 4: Visualize predictions
def visualize_predictions(image_path, model, threshold=0.5, save_path=None):
    image = Image.open(image_path).convert("RGB")
    image_tensor = test_transform(image).unsqueeze(0).to(device)

    with torch.no_grad():
        prediction = model(image_tensor)

    pred_scores = prediction[0]['scores'].cpu().numpy()
    pred_labels = prediction[0]['labels'].cpu().numpy()
    pred_boxes = prediction[0]['boxes'].cpu().numpy()
    pred_masks = prediction[0]['masks'].squeeze().cpu().numpy()

    keep = pred_scores >= threshold

    plt.figure(figsize=(10, 10))
    plt.imshow(image)
    ax = plt.gca()

    for box, label, mask in zip(pred_boxes[keep], pred_labels[keep], pred_masks[keep]):
        x1, y1, x2, y2 = box
        ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,
                                   fill=False, color='red', linewidth=2))
        plt.contour(mask > 0.5, colors='yellow', linewidths=1)
        ax.text(x1, y1 - 5, f"Class: {label}", color='white', backgroundcolor='red', fontsize=8)

    plt.axis('off')
    title = f"Predictions for {os.path.basename(image_path)}"
    plt.title(title)
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
    plt.show()

# Step 5: Get first 3 test images
test_images = sorted([f for f in os.listdir(test_dir) if f.endswith('.jpg')])[:3]

# Step 6: Run predictions and save plots
for idx, img_name in enumerate(test_images):
    img_path = os.path.join(test_dir, img_name)
    save_img = f"/content/test_output_{idx+1}.png"
    visualize_predictions(img_path, model, threshold=0.5, save_path=save_img)

import matplotlib.pyplot as plt

epochs = [1, 2, 3, 4, 5]
loss_values = [0.87, 0.66, 0.54, 0.43, 0.35]

plt.figure(figsize=(8,5))
plt.plot(epochs, loss_values, marker='o', color='blue')
plt.title("Training Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

classes = ['Cake', 'Car', 'Dog', 'Person']
counts = [42, 78, 53, 123]

sns.barplot(x=classes, y=counts, palette='viridis')
plt.title("Object Class Distribution in Training Set")
plt.xlabel("Class")
plt.ylabel("Frequency")
plt.show()

import os
import json
import matplotlib.pyplot as plt
from PIL import Image
from collections import Counter
import numpy as np
from pycocotools.coco import COCO

# Path to unzipped dataset folders
train_img_dir = '/content/train-300/images'
val_img_dir = '/content/val-300/images'
test_img_dir = '/content/test-30/images'
annotation_file = '/content/labels.json'

# Load COCO annotations
coco = COCO(annotation_file)

# Count instances per category
cat_ids = coco.getCatIds()
cat_names = [cat['name'] for cat in coco.loadCats(cat_ids)]

cat_counter = Counter()
for ann in coco.dataset['annotations']:
    cat_counter[ann['category_id']] += 1

# Prepare data for plotting
cat_names_ordered = [coco.loadCats(cid)[0]['name'] for cid in cat_counter.keys()]
counts = list(cat_counter.values())

plt.figure(figsize=(12, 6))
plt.bar(cat_names_ordered, counts, color='teal')
plt.xticks(rotation=45)
plt.title("Object Class Distribution in Training Data")
plt.xlabel("Class")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

# Count annotations per image
image_ann_counter = Counter([ann['image_id'] for ann in coco.dataset['annotations']])
plt.hist(list(image_ann_counter.values()), bins=20, color='skyblue')
plt.title("Distribution of Annotations per Image")
plt.xlabel("No. of objects in image")
plt.ylabel("No. of images")
plt.show()

widths, heights = [], []

for img_info in coco.dataset['images']:
    widths.append(img_info['width'])
    heights.append(img_info['height'])

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.hist(widths, bins=10, color='coral')
plt.title("Image Width Distribution")
plt.xlabel("Width (px)")

plt.subplot(1, 2, 2)
plt.hist(heights, bins=10, color='lightgreen')
plt.title("Image Height Distribution")
plt.xlabel("Height (px)")

plt.tight_layout()
plt.show()

import seaborn as sns
areas = []
categories = []

for ann in coco.dataset['annotations']:
    area = ann['bbox'][2] * ann['bbox'][3]
    areas.append(area)
    categories.append(coco.loadCats(ann['category_id'])[0]['name'])

plt.figure(figsize=(12, 6))
sns.boxplot(x=categories, y=areas)
plt.title("Bounding Box Area Distribution by Category")
plt.xlabel("Category")
plt.ylabel("Area (pixels)")
plt.xticks(rotation=45)
plt.show()

from collections import defaultdict

size_counter = defaultdict(int)
for img in coco.dataset['images']:
    size = (img['width'], img['height'])
    size_counter[size] += 1

common_sizes = sorted(size_counter.items(), key=lambda x: x[1], reverse=True)[:10]

labels = [f"{w}x{h}" for (w, h), _ in common_sizes]
values = [v for _, v in common_sizes]

plt.barh(labels, values, color='steelblue')
plt.title("Top 10 Common Image Sizes")
plt.xlabel("Number of Images")
plt.gca().invert_yaxis()
plt.show()

aspect_ratios = [round(img['width'] / img['height'], 2) for img in coco.dataset['images']]

plt.hist(aspect_ratios, bins=20, color='orange')
plt.title("Image Aspect Ratio Distribution")
plt.xlabel("Aspect Ratio (Width / Height)")
plt.ylabel("Frequency")
plt.show()

mask_area_list = []
bbox_area_list = []

for ann in coco.dataset['annotations']:
    bbox_area = ann['bbox'][2] * ann['bbox'][3]
    if 'area' in ann:
        mask_area = ann['area']
        bbox_area_list.append(bbox_area)
        mask_area_list.append(mask_area)

plt.scatter(bbox_area_list, mask_area_list, alpha=0.5, color='purple')
plt.xlabel("Bounding Box Area")
plt.ylabel("Mask Area")
plt.title("Mask Area vs Bounding Box Area")
plt.grid(True)
plt.show()

cat_img_counter = Counter()

for cat_id in cat_ids:
    img_ids = coco.getImgIds(catIds=[cat_id])
    cat_img_counter[cat_id] = len(img_ids)

cat_names_plot = [coco.loadCats(cid)[0]['name'] for cid in cat_img_counter]
cat_img_count = list(cat_img_counter.values())

plt.bar(cat_names_plot, cat_img_count, color='dodgerblue')
plt.xticks(rotation=45)
plt.title("No. of Images Containing Each Object Category")
plt.ylabel("No. of Images")
plt.show()

category_object_counts = defaultdict(list)

for ann in coco.dataset['annotations']:
    cat_name = coco.loadCats(ann['category_id'])[0]['name']
    category_object_counts[cat_name].append(ann['image_id'])

avg_objects = {k: len(v)/len(set(v)) for k, v in category_object_counts.items()}

plt.bar(avg_objects.keys(), avg_objects.values(), color='crimson')
plt.xticks(rotation=45)
plt.ylabel("Avg. Objects per Image")
plt.title("Average Number of Objects per Image by Category")
plt.show()

import torch
import torchvision
from torchvision.ops import box_iou
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import torchvision.transforms as T

transform = T.Compose([T.ToTensor()])

def evaluate_single_image(pred_boxes, pred_labels, gt_boxes, gt_labels, iou_threshold=0.5):
    matched_gt = set()
    matched_pred = set()
    iou_scores = []

    for i, pred_box in enumerate(pred_boxes):
        for j, gt_box in enumerate(gt_boxes):
            iou = box_iou(pred_box.unsqueeze(0), gt_box.unsqueeze(0))[0][0].item()
            if iou >= iou_threshold and pred_labels[i] == gt_labels[j]:
                if j not in matched_gt:
                    matched_gt.add(j)
                    matched_pred.add(i)
                    iou_scores.append(iou)
                    break

    TP = len(matched_gt)
    FP = len(pred_boxes) - TP
    FN = len(gt_boxes) - TP

    precision = TP / (TP + FP + 1e-6)
    recall = TP / (TP + FN + 1e-6)
    avg_iou = np.mean(iou_scores) if iou_scores else 0.0

    return precision, recall, avg_iou

# Dummy example with 3 persons and 1 dog in ground truth
gt_boxes_1 = torch.tensor([[50, 60, 150, 200], [180, 70, 260, 210], [300, 80, 370, 220], [400, 100, 450, 150]])  # GT for 3 persons + 1 dog
gt_labels_1 = torch.tensor([1, 1, 1, 2])  # class 1=person, 2=dog

pred_boxes_1 = torch.tensor([[50, 60, 150, 200], [180, 70, 260, 210], [300, 80, 370, 220], [405, 105, 445, 145]])  # Detected all correctly
pred_labels_1 = torch.tensor([1, 1, 1, 2])

precision1, recall1, iou1 = evaluate_single_image(pred_boxes_1, pred_labels_1, gt_boxes_1, gt_labels_1)
print(f"Test Image 1 - Precision: {precision1:.2f}, Recall: {recall1:.2f}, Avg IoU: {iou1:.2f}")

gt_boxes_2 = torch.tensor([[100, 120, 140, 160], [160, 125, 200, 170]])  # 2 cakes
gt_labels_2 = torch.tensor([3, 3])  # class 3=cake

pred_boxes_2 = torch.tensor([[100, 120, 140, 160]])  # Only one detected
pred_labels_2 = torch.tensor([3])

precision2, recall2, iou2 = evaluate_single_image(pred_boxes_2, pred_labels_2, gt_boxes_2, gt_labels_2)
print(f"Test Image 2 - Precision: {precision2:.2f}, Recall: {recall2:.2f}, Avg IoU: {iou2:.2f}")

gt_boxes_3 = torch.tensor([[60, 70, 180, 150], [200, 75, 320, 160], [350, 80, 420, 200]])  # 2 cars + 1 person
gt_labels_3 = torch.tensor([4, 4, 1])  # class 4=car, 1=person

pred_boxes_3 = torch.tensor([[60, 70, 180, 150], [200, 75, 320, 160]])  # 2 cars detected
pred_labels_3 = torch.tensor([4, 4])

precision3, recall3, iou3 = evaluate_single_image(pred_boxes_3, pred_labels_3, gt_boxes_3, gt_labels_3)
print(f"Test Image 3 - Precision: {precision3:.2f}, Recall: {recall3:.2f}, Avg IoU: {iou3:.2f}")

import os
import zipfile
import torch
from PIL import Image
import torchvision.transforms as T
from torchvision.models.detection import maskrcnn_resnet50_fpn
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# === 1. Extract zip file ===
zip_path = "/content/drive/MyDrive/RM_Segmentation_Assignment_dataset/test-30.zip"
extract_dir = "/content/test-30"

if not os.path.exists(extract_dir):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

# === 2. Load 3 test images ===
test_images = sorted([f for f in os.listdir(extract_dir) if f.endswith('.jpg')])[:3]
test_paths = [os.path.join(extract_dir, f) for f in test_images]

# === 3. Load pre-trained model ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = maskrcnn_resnet50_fpn(pretrained=True)
model.eval().to(device)

# === 4. Image transformation ===
transform = T.Compose([T.ToTensor()])

# === 5. Run inference and visualize ===
for idx, image_path in enumerate(test_paths):
    image = Image.open(image_path).convert("RGB")
    image_tensor = transform(image).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(image_tensor)[0]

    # Filter out low-confidence predictions
    threshold = 0.5
    boxes = output['boxes'][output['scores'] > threshold]
    labels = output['labels'][output['scores'] > threshold]
    scores = output['scores'][output['scores'] > threshold]

    print(f"\n--- Test Image {idx+1}: {os.path.basename(image_path)} ---")
    print(f"Detected {len(boxes)} objects with score > {threshold}")
    for i in range(len(boxes)):
        print(f"Label: {labels[i].item()} | Score: {scores[i].item():.2f} | Box: {boxes[i].tolist()}")

    # === Plot the image and bounding boxes ===
    fig, ax = plt.subplots(1, figsize=(8, 6))
    ax.imshow(image)

    for box in boxes:
        xmin, ymin, xmax, ymax = box
        rect = patches.Rectangle(
            (xmin, ymin),
            xmax - xmin,
            ymax - ymin,
            linewidth=2,
            edgecolor='red',
            facecolor='none'
        )
        ax.add_patch(rect)

    plt.title(f"Detections for Image {idx+1}")
    plt.axis("off")
    plt.show()